{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9ec677",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Before start, you may need to learn something about super-resolution by collecting resources from Internet\n",
    "### Super-resolution is an easy task and serves as a low-level task of computer vision\n",
    "### The basic concept can be summarized in one sentence: reconstructing low resolution images into higher resolution images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ffed71",
   "metadata": {},
   "source": [
    "## Load data    \n",
    "You can also use prepare.ipynb to generate training data from original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2abe9809-4bd1-4de6-acff-a29cef5286e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn,optim\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e74a55b-14b7-4c4d-aa48-783e1c3219d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, root, upscale_factor):\n",
    "        super(SRDataset, self).__init__()\n",
    "        self.hr_path = os.path.join(root, 'train_64')\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.hr_filenames = sorted(os.listdir(self.hr_path))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        hr_image = cv2.imread(os.path.join(self.hr_path, self.hr_filenames[index]))\n",
    "        hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = hr_image.shape\n",
    "\n",
    "        ## make sure same demension\n",
    "        h -= h % self.upscale_factor\n",
    "        w -= w % self.upscale_factor\n",
    "        hr_image = hr_image[:h, :w]\n",
    "\n",
    "        lr_image = cv2.resize(hr_image, (int(w // self.upscale_factor),int(h // self.upscale_factor)), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "\n",
    "        if random.random() > 0.5:  \n",
    "            lr_image = cv2.flip(lr_image, 1)\n",
    "            hr_image = cv2.flip(hr_image, 1)\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        lr_image = transform(lr_image)\n",
    "        hr_image = transform(hr_image)\n",
    "\n",
    "\n",
    "\n",
    "        return lr_image, hr_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hr_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03046720-0094-4703-8b65-0819b000f7e7",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ab9a65-b566-47b9-9251-ba2b9c57462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "upscale= 4\n",
    "train_dataset = SRDataset(root='./data/PlantSR_dataset/', upscale_factor=upscale)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c592409e-cd55-44d2-8554-8dbbbf37d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the original useage of PlantSRv1, you need to adjust them accoarding to your PlantSRv2\n",
    "\n",
    "## upscale = 2/3/4\n",
    "from models.PlantSRv2 import PlantSR\n",
    "\n",
    "outPath = \"outputs\"\n",
    "lr = 1e-4  \n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "if upscale == 4:\n",
    "   model = PlantSR(\n",
    "                 scale=upscale,\n",
    "                 img_size=64,\n",
    "                 num_channels=3,\n",
    "                 num_features=96,\n",
    "                 depth=[4,4,4,4,4],\n",
    "                 num_heads=[4,4,4,4,4],\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.1,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 use_chk=False,\n",
    "                 img_range=1.,\n",
    "                 resi_connection='1conv',\n",
    "                 split_size=[8,8],\n",
    "                 c_ratio=0.5,ffn_scale=2,n_blocks=4)\n",
    "if upscale == 2:\n",
    "    model = PlantSR(\n",
    "                 scale=upscale,\n",
    "                 img_size=64,\n",
    "                 num_channels=3,\n",
    "                 num_features=32,\n",
    "                 depth=[2,2,2,2],\n",
    "                 num_heads=[2,2,2,2],\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.1,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 use_chk=False,\n",
    "                 img_range=1.,\n",
    "                 resi_connection='1conv',\n",
    "                 split_size=[8,8],\n",
    "                 c_ratio=0.5,ffn_scale=2,n_blocks=4)\n",
    "if upscale == 3:\n",
    "    model = PlantSR(scale=upscale, num_channels=3, num_features=64,n_resgroups=16,n_resblocks=4,reduction=16,ffn_scale=2,n_blocks=5)\n",
    "model.to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2020c06-acfd-41e0-9552-c87bf5da2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the pretrained model (if have one)\n",
    "\n",
    "# model_path = 'ckpts/PlantSR_x2_best.pth'\n",
    "# model.load_state_dict(torch.load(model_path), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dfd32df-a578-4c40-bb84-171355b77609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Batch [17148/17148], Loss: 0.0261\n",
      "\n",
      "Epoch [2/12], Batch [17148/17148], Loss: 0.0158\n",
      "\n",
      "Epoch [3/12], Batch [17148/17148], Loss: 0.0315\n",
      "\n",
      "Epoch [4/12], Batch [17148/17148], Loss: 0.0251\n",
      "\n",
      "Epoch [5/12], Batch [17148/17148], Loss: 0.0283\n",
      "\n",
      "Epoch [6/12], Batch [17148/17148], Loss: 0.0305\n",
      "\n",
      "Epoch [7/12], Batch [17148/17148], Loss: 0.0154\n",
      "\n",
      "Epoch [8/12], Batch [17148/17148], Loss: 0.0169\n",
      "\n",
      "Epoch [9/12], Batch [17148/17148], Loss: 0.0205\n",
      "\n",
      "Epoch [10/12], Batch [17148/17148], Loss: 0.0233\n",
      "\n",
      "Epoch [11/12], Batch [17148/17148], Loss: 0.0313\n",
      "\n",
      "Epoch [12/12], Batch [17148/17148], Loss: 0.0253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "\n",
    "start_epoch = 0\n",
    "num_epochs =12\n",
    "\n",
    "for epoch in range(start_epoch,num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (lr_images, hr_images) in enumerate(train_loader):\n",
    "        lr_images = lr_images.to(device)\n",
    "        hr_images = hr_images.to(device)\n",
    "\n",
    "        sr_images = model(lr_images.float())\n",
    "\n",
    "        loss = criterion(sr_images, hr_images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx+1) % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'\n",
    "                             .format(epoch+1, num_epochs, batch_idx+1, len(train_loader), loss.item()))\n",
    "            sys.stdout.flush()\n",
    "        # if(batch_idx%10000) == 0:\n",
    "        #     torch.save(model.state_dict(), 'outputs/PlantSR_x2_{}_{}.pth'.format(batch_idx,epoch+1))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        torch.save(model.state_dict(), 'outputs/PlantSRv2_x4_{}.pth'.format(epoch+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da02ad7f-d48f-4b33-9263-586eb022bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "upscale= 4\n",
    "train_dataset = SRDataset(root='./data/PlantSR_dataset/', upscale_factor=upscale)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2dc73e1-f88b-40fe-8271-b64d72365838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_14192\\3227793628.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path), strict=True)\n"
     ]
    }
   ],
   "source": [
    "from models.PlantSRv2 import PlantSR\n",
    "\n",
    "outPath = \"outputs\"\n",
    "lr = 1e-4  \n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "if upscale == 4:\n",
    "   model = PlantSR(\n",
    "                 scale=upscale,\n",
    "                 img_size=64,\n",
    "                 num_channels=3,\n",
    "                 num_features=96,\n",
    "                 depth=[4,4,4,4,4],\n",
    "                 num_heads=[4,4,4,4,4],\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.1,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 use_chk=False,\n",
    "                 img_range=1.,\n",
    "                 resi_connection='1conv',\n",
    "                 split_size=[8,8],\n",
    "                 c_ratio=0.5,ffn_scale=2,n_blocks=4)\n",
    "if upscale == 2:\n",
    "    model = PlantSR(\n",
    "                 scale=upscale,\n",
    "                 img_size=64,\n",
    "                 num_channels=3,\n",
    "                 num_features=32,\n",
    "                 depth=[2,2,2,2],\n",
    "                 num_heads=[2,2,2,2],\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.1,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 use_chk=False,\n",
    "                 img_range=1.,\n",
    "                 resi_connection='1conv',\n",
    "                 split_size=[8,8],\n",
    "                 c_ratio=0.5,ffn_scale=2,n_blocks=4)\n",
    "if upscale == 3:\n",
    "    model = PlantSR(scale=upscale, num_channels=3, num_features=64,n_resgroups=16,n_resblocks=4,reduction=16,ffn_scale=2,n_blocks=5)\n",
    "model_path = r'outputs/PlantSRv2_x4_28.pth'\n",
    "model.load_state_dict(torch.load(model_path), strict=True)\n",
    "model.to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960e2b0e-6f17-40ee-9b58-42fcc074e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/36], Batch [17148/17148], Loss: 0.0161\n",
      "\n",
      "Epoch [30/36], Batch [17148/17148], Loss: 0.0218\n",
      "\n",
      "Epoch [31/36], Batch [17148/17148], Loss: 0.0231\n",
      "\n",
      "Epoch [32/36], Batch [17148/17148], Loss: 0.0210\n",
      "\n",
      "Epoch [33/36], Batch [17148/17148], Loss: 0.0204\n",
      "\n",
      "Epoch [34/36], Batch [17148/17148], Loss: 0.0132\n",
      "\n",
      "Epoch [35/36], Batch [17148/17148], Loss: 0.0238\n",
      "\n",
      "Epoch [36/36], Batch [17148/17148], Loss: 0.0262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "num_epochs = 8  # 继续训练8轮\n",
    "start_epoch = 28  # 之前已经训练了12轮\n",
    "\n",
    "# 继续训练的代码\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (lr_images, hr_images) in enumerate(train_loader):\n",
    "        lr_images = lr_images.to(device)\n",
    "        hr_images = hr_images.to(device)\n",
    "\n",
    "        sr_images = model(lr_images.float())\n",
    "\n",
    "        loss = criterion(sr_images, hr_images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx+1) % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'\n",
    "                             .format(epoch+1, start_epoch + num_epochs, batch_idx+1, len(train_loader), loss.item()))\n",
    "            sys.stdout.flush()\n",
    "        # if(batch_idx%10000) == 0:\n",
    "        #     torch.save(model.state_dict(), 'outputs/PlantSR_x2_{}_{}.pth'.format(batch_idx,epoch+1))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        torch.save(model.state_dict(), 'outputs/PlantSRv2_x4_{}.pth'.format(epoch+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8bcd2",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf372cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13532\\879830013.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path), strict=True)\n"
     ]
    }
   ],
   "source": [
    "from models.PlantSRv2 import PlantSR\n",
    "# from   PLtest import PlantSR\n",
    "import torch\n",
    "\n",
    "upscale = 4\n",
    "device = 'cuda'\n",
    "# model_path = r'outputs/PlantSR_x4_20000_10.pth'\n",
    "model_path = r'outputs/PlantSRv2_x4_30.pth'\n",
    "device = torch.device('cuda:0')\n",
    "if upscale == 4:\n",
    "    model = PlantSR(scale=upscale,\n",
    "                 img_size=64,\n",
    "                 num_channels=3,\n",
    "                 num_features=96,\n",
    "                 depth=[4,4,4,4,4],\n",
    "                 num_heads=[4,4,4,4,4],\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.1,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 use_chk=False,\n",
    "                 img_range=1.,\n",
    "                 resi_connection='1conv',\n",
    "                 split_size=[8,8],\n",
    "                 c_ratio=0.5,ffn_scale=2,n_blocks=4)\n",
    "if upscale == 2:\n",
    "    model = PlantSR(scale=upscale,\n",
    "                 img_size=64,\n",
    "                 num_channels=3,\n",
    "                 num_features=32,\n",
    "                 depth=[2,2,2,2],\n",
    "                 num_heads=[2,2,2,2],\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.1,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 use_chk=False,\n",
    "                 img_range=1.,\n",
    "                 resi_connection='1conv',\n",
    "                 split_size=[8,8],\n",
    "                 c_ratio=0.5,ffn_scale=2,n_blocks=4)\n",
    "if upscale == 3:\n",
    "    model = PlantSR(scale=upscale,num_features=64,n_resgroups=16,n_resblocks=4,reduction=16)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path), strict=True)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dfea863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv2\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from calulate_psnr_ssim import *\n",
    "import os\n",
    "\n",
    "test_psnr = 0\n",
    "test_ssim = 0\n",
    "image_count = 0\n",
    "\n",
    "test_path = \"./data/PlantSR_dataset/test\"\n",
    "\n",
    "for filename in os.listdir(test_path):\n",
    "    if filename.endswith((\".png\",\".jpg\")):\n",
    "        image_count+=1\n",
    "        print(image_count)\n",
    "        file_path = os.path.join(test_path, filename)\n",
    "        \n",
    "        hr_img = cv2.imread(file_path, cv2.IMREAD_COLOR).astype(np.float32) \n",
    "        h, w, _ = hr_img.shape\n",
    "\n",
    "        ## make sure same dimension\n",
    "        h -= h % upscale\n",
    "        w -= w % upscale\n",
    "        hr_img = hr_img[:h, :w]\n",
    "        \n",
    "        lr_image = cv2.resize(hr_img, (w // upscale, h // upscale), interpolation=cv2.INTER_LINEAR) \n",
    "        lr_image = lr_image/255.\n",
    "        lr_image = torch.from_numpy(np.transpose(lr_image[:, :, [2, 1, 0]],\n",
    "                                                (2, 0, 1))).float()\n",
    "        lr_image = lr_image.unsqueeze(0).to(device)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(lr_image)\n",
    "\n",
    "        output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "        output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))\n",
    "        output = (output * 255.0)\n",
    "\n",
    "        # if (output.shape[2] != hr_img.shape[2]) or (output.shape[3] != hr_img.shape[3]):\n",
    "        #     output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\n",
    "            \n",
    "        psnr = calc_psnr(hr_img, output)\n",
    "        ssim = calc_ssim(hr_img, output)\n",
    "        test_psnr += psnr\n",
    "        test_ssim += ssim\n",
    "\n",
    "test_psnr = test_psnr/image_count\n",
    "test_ssim = test_ssim/image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a46cb620",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test psnr: 33.69\n",
      "test ssim: 0.9049\n"
     ]
    }
   ],
   "source": [
    "print('test psnr: {:.2f}'.format(test_psnr))\n",
    "print('test ssim: {:.4f}'.format(test_ssim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ecb3f8",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c4f9d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12548\\2332085247.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path), strict=True)\n"
     ]
    }
   ],
   "source": [
    "from models.PlantSRv2 import PlantSR\n",
    "import torch\n",
    "\n",
    "upscale = 2\n",
    "device = 'cuda'\n",
    "model_path = r'outputs/PlantSRv2_x2_8.pth'\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "if upscale == 4:\n",
    "    model = PlantSR(scale=upscale,num_features=96,n_resgroups=16,n_resblocks=4,reduction=16)\n",
    "if upscale == 2:\n",
    "    model = PlantSR(scale=upscale,\n",
    "                 img_size=64,\n",
    "                 num_channels=3,\n",
    "                 num_features=32,\n",
    "                 depth=[2,2,2,2,2,2],\n",
    "                 num_heads=[2,2,2,2,2,2],\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.1,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 use_chk=False,\n",
    "                 img_range=1.,\n",
    "                 resi_connection='1conv',\n",
    "                 split_size=[8,8],\n",
    "                 c_ratio=0.5,ffn_scale=2,n_blocks=4)\n",
    "if upscale == 3:\n",
    "    model = PlantSR(scale=upscale,num_features=64,n_resgroups=16,n_resblocks=4,reduction=16)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path), strict=True)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ad519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "input_folder = \"G:/6666/duibizu/PlantSR/data/PlantSR_dataset/test\"\n",
    "output_folder = \"G:/6666/duibizu/PlantSR/data/PlantSR_dataset/YourDatax2\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR).astype(np.float32) /255.\n",
    "        h, w, _ = img.shape\n",
    "        img = torch.from_numpy(np.transpose(img[:, :, [2, 1, 0]], (2, 0, 1))).float()\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "\n",
    "        output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "        output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))\n",
    "        output = (output * 255.0).round().astype(np.uint8)\n",
    "\n",
    "        save_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        cv2.imwrite(save_path, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a1819",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
